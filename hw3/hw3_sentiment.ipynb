{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chakryaros/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chakryaros/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - bag-of-words baseline\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exampleText' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-83ca1508523c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;31m# do the things that you need to with your base class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m     \u001b[0msa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexampleText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m#get the development text from file and calculate the classify each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exampleText' is not defined"
     ]
    }
   ],
   "source": [
    "# STEP 1: rename this file to hw3_sentiment.py\n",
    "\n",
    "# feel free to include more imports as needed here\n",
    "# these are the ones that we used for the base model\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Your name and file comment here:\n",
    "Name : Chakrya Ros\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Cite your sources here:\n",
    " - https://gist.github.com/sebleier/554280\n",
    " - https://github.com/llSourcell/logistic_regression/blob/master/Sentiment%20analysis%20with%20Logistic%20Regression.ipynb\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Implement your functions that are not methods of the Sentiment Analysis class here\n",
    "\"\"\"\n",
    "def generate_tuples_from_file(training_file_path):\n",
    "    file = open(training_file_path, 'r')\n",
    "    sentences = file.read()\n",
    "    file.close()\n",
    "    list_word = []\n",
    "    tuple_word = ()\n",
    "    wordsList = sentences.split(\"\\n\")\n",
    "   \n",
    "    for words in wordsList:\n",
    "        word = words.split('\\t')\n",
    "        if len(word) == 3:\n",
    "            tupleWord = (word[0],word[1],word[2])\n",
    "            list_word.append(tupleWord)\n",
    "    return list_word\n",
    "        \n",
    "\n",
    "def precision(gold_labels, classified_labels):\n",
    "    truePostive = 0\n",
    "    falsePostive = 0\n",
    "    for i in range(len(gold_labels)):\n",
    "        if gold_labels[i] == str(1) and classified_labels[i] == str(1):\n",
    "            truePostive +=1\n",
    "        elif gold_labels[i] == str(0) and classified_labels[i] == str(1):\n",
    "            falsePostive +=1\n",
    "#             print(\"falsePostive,\",falsePostive)\n",
    "    return (truePostive/(truePostive+falsePostive))\n",
    "\n",
    " \n",
    "def recall(gold_labels, classified_labels):\n",
    "    truePostive = 0\n",
    "    falseNegative = 0\n",
    "    for i in range(len(gold_labels)):\n",
    "        if gold_labels[i] == '1' and classified_labels[i] == '1':\n",
    "            truePostive +=1\n",
    "        elif gold_labels[i] == '1' and classified_labels[i] == '0':\n",
    "            falseNegative +=1\n",
    "    return (truePostive/(truePostive+falseNegative))\n",
    "\n",
    "\n",
    "def f1(gold_labels, classified_labels):\n",
    "    prec = precision(gold_labels, classified_labels)\n",
    "    recal = recall(gold_labels, classified_labels)\n",
    "    f1 = 2 * ((prec*recal)/(prec+recal))\n",
    "    return f1\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Implement any other non-required functions here\n",
    "\"\"\"\n",
    "\n",
    "#helper function to generate tuples from list\n",
    "def generate_tuples_from_train(k_fold_list):\n",
    "    list_word = []\n",
    "    tuple_word = ()\n",
    "    for idx,data in enumerate(k_fold_list):\n",
    "        for doc in data:\n",
    "            word = doc.split('\\t')\n",
    "            if len(word) == 3:\n",
    "                tupleWord = (word[0],word[1],word[2])\n",
    "                list_word.append(tupleWord)\n",
    "    return list_word\n",
    "\n",
    "#helper function to generate tuples from list\n",
    "def generate_tuples_from_test(testSet):\n",
    "    list_word = []\n",
    "    tuple_word = ()\n",
    "    for word in testSet:\n",
    "        word = word.split('\\t')\n",
    "        if len(word) == 3:\n",
    "            tupleWord = (word[0],word[1],word[2])\n",
    "            list_word.append(tupleWord)\n",
    "    return list_word\n",
    "\n",
    "#helper function for combine training dat with given development\n",
    "def createNewDataset(train_data, devData):\n",
    "    train_file = open(train_data, 'r')\n",
    "    dev_file = open(devData,'r')\n",
    "    train_data_read = train_file.read()\n",
    "    devData_read = dev_file.read()\n",
    "\n",
    "    train_file.close()\n",
    "    dev_file.close()\n",
    "\n",
    "    train_data_read = train_data_read.split('\\n')\n",
    "    devData_read = devData_read.split('\\n')\n",
    "    combineData = train_data_read + devData_read\n",
    "    combineData.pop(-1)\n",
    "    return combineData\n",
    "\n",
    "# print the recall and precision and f1 score from k fold\n",
    "def print_result_score(improvedModel, k_dataset, k):\n",
    "    k_dataset_copy = k_dataset\n",
    "    for idx in range(k):\n",
    "        test = k_dataset_copy.pop(idx)\n",
    "        exampleDev_text = generate_tuples_from_test(test)\n",
    "        train = k_dataset_copy\n",
    "        k_dataset_copy.append(test)\n",
    "        exampleText = generate_tuples_from_train(train)\n",
    "        \n",
    "        #train each k fold\n",
    "        improvedModel.train(exampleText)\n",
    "        \n",
    "        #get the development text from file and calculate the classify each document\n",
    "        dev_list = []\n",
    "        gold_labels = []\n",
    "        for i in range(len(exampleDev_text)):\n",
    "            dev_list.append(exampleDev_text[i][1])\n",
    "            gold_labels.append(exampleDev_text[i][2])\n",
    "        print(gold_labels)\n",
    "\n",
    "\n",
    "        #classify for each document\n",
    "        classify_labels = []\n",
    "        for data in dev_list:\n",
    "            classify_labels.append(improvedModel.classify(data))\n",
    "        print(classify_labels)\n",
    "        print(\"SentimanetAnaysisImproved Model\")\n",
    "        print(\"Recall    : {}\".format(recall(gold_labels, classify_labels)))\n",
    "        print(\"Precision : {}\".format(precision(gold_labels, classify_labels)))\n",
    "        print(\"F1 Score  : {}\".format(f1(gold_labels, classify_labels)))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "implement your SentimentAnalysis class here\n",
    "\"\"\"\n",
    "class SentimentAnalysis:\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "    # do whatever you need to do to set up your class here\n",
    "        self.tupleWordPostive = []  \n",
    "        self.tupleWordNegative = []\n",
    "        self.NumDocu = 0          #number document\n",
    "        self.NumPositive = 0      #number one class\n",
    "        self.NumNegative = 0      # number zero class\n",
    "        self.freqPos = {}         # frequency words in postive class\n",
    "        self.freqNeg = {}         # frequency words in negative class\n",
    "        self.tokenPostive = 0     #number words in postive class\n",
    "        self.tokenNegative = 0    #numver words in negative class\n",
    "        self.vocab = 0.0          # length of vocabulary\n",
    "        self.vocab_list = []      #list of word in all document\n",
    "        self.prior = {}           #calculate the probability of class\n",
    "        self.likelihood = {}      #calculate the probability of word given class\n",
    "        \n",
    "\n",
    "    def train(self, examples):\n",
    "        for i in range(len(examples)):\n",
    "            if examples[i][2] == '1':\n",
    "                self.NumPositive += 1 #count positive class\n",
    "                self.tupleWordPostive.append(examples[i][1])\n",
    "            else:\n",
    "                self.NumNegative += 1 #count negative class\n",
    "                self.tupleWordNegative.append(examples[i][1])\n",
    "\n",
    "        \n",
    "        #get the words and frequency word and store in frequency positive dictionary\n",
    "        for sentence in self.tupleWordPostive:\n",
    "            words = sentence.split()\n",
    "            for w in words:\n",
    "                if w in self.freqPos:\n",
    "                    self.freqPos[w] +=1\n",
    "                else:\n",
    "                    self.freqPos[w] = 1\n",
    "            for w in words:\n",
    "                if w in self.vocab_list:\n",
    "                    continue\n",
    "                else:\n",
    "                    self.vocab_list.append(w)\n",
    "                \n",
    "        #get the words and frequency word and store in frequency negative dictionary\n",
    "        for sentence in self.tupleWordNegative:\n",
    "            words = sentence.split()\n",
    "            for w in words:\n",
    "                if w in self.freqNeg:\n",
    "                    self.freqNeg[w] +=1\n",
    "                else:\n",
    "                    self.freqNeg[w] = 1\n",
    "            for w in words:\n",
    "                if w in self.vocab_list:\n",
    "                    continue\n",
    "                else:\n",
    "                    self.vocab_list.append(w)\n",
    "                    \n",
    "        #get tokens word from frequecy postive \n",
    "        self.tokenPostive = sum(self.freqPos.values())\n",
    "        \n",
    "        #get tokens word from frequecy Negative \n",
    "        self.tokenNegative = sum(self.freqNeg.values())\n",
    "        \n",
    "        #all vocabulatary in both class\n",
    "        self.vocab = len(self.vocab_list)\n",
    "        \n",
    "        #number of document\n",
    "        self.NumDocu = len(examples)\n",
    "        \n",
    "       \n",
    "        #calculate the prior of each class and add into dictionary\n",
    "        self.prior['1'] = self.NumPositive/self.NumDocu\n",
    "        self.prior['0'] = self.NumNegative/self.NumDocu\n",
    "        \n",
    "        #calcualte the likelihood of each class and add into dictionatry\n",
    "        for word in self.vocab_list:\n",
    "            if word in self.freqPos:\n",
    "                likelihood_one = (self.freqPos[word] + 1) / (self.tokenPostive + self.vocab)\n",
    "                self.likelihood[(word,'1')] = likelihood_one\n",
    "            else: \n",
    "                likelihood_one = ( 0 + 1) / (self.tokenPostive + self.vocab)\n",
    "                self.likelihood[(word,'1')] = likelihood_one\n",
    "\n",
    "            \n",
    "            if word in self.freqNeg:\n",
    "                likelihood_zero = (self.freqNeg[word] + 1) / (self.tokenNegative + self.vocab)\n",
    "                self.likelihood[(word,'0')] = likelihood_zero\n",
    "            else:\n",
    "                likelihood_zero = (0 + 1) / (self.tokenNegative + self.vocab)\n",
    "                self.likelihood[(word,'0')] = likelihood_zero\n",
    "        \n",
    "        \n",
    "\n",
    "    # calculate the score from the data\n",
    "    #return the probability of each class\n",
    "    def score(self, data):\n",
    "        words = data.split()\n",
    "        p_data_one = 0.0\n",
    "        p_data_zero = 0.0\n",
    "        likelihood_one = 0\n",
    "        likelihood_zero = 0\n",
    "        for w in words:\n",
    "            if w in self.vocab_list:\n",
    "                likelihood_one += np.log(self.likelihood[(w,'1')])\n",
    "                likelihood_zero += np.log(self.likelihood[(w,'0')])\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        p_data_one = np.exp(np.log(self.prior['1']) + likelihood_one)\n",
    "        p_data_zero = np.exp(np.log(self.prior['0']) + likelihood_zero)\n",
    "        \n",
    "        return { '1' : p_data_one, '0' : p_data_zero}\n",
    "                \n",
    "                \n",
    "    def classify(self, data):\n",
    "        dict_class = self.score(data)\n",
    "        if dict_class['1'] > dict_class['0']:\n",
    "            return '1'\n",
    "        else:\n",
    "            return '0'\n",
    "        \n",
    "    # store feature and its value into list of tuple\n",
    "    def featurize(self, data):\n",
    "        words = data.split()\n",
    "        word_vectors = []\n",
    "        for w in words:\n",
    "            word_vectors.append((w,True))\n",
    "#         print(word_vectors)\n",
    "        return word_vectors\n",
    "            \n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Naive Bayes - bag-of-words baseline\"\n",
    "\n",
    "\n",
    "class SentimentAnalysisImproved:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.vocab_list = Counter()          #list of word in all document\n",
    "        self.vocab_reduced = Counter()      # reduce stop words\n",
    "        self.NumClass = Counter()\n",
    "        self.prior = {}                     #calculate the probability of class\n",
    "        self.likelihood = {}                #calculate the probability of word given class\n",
    "        self.freqPos = Counter()\n",
    "        self.freqNeg = Counter()\n",
    "        self.tokenPostive = 0\n",
    "        self.tokenNegative = 0\n",
    "        \n",
    " \n",
    "    #helper function to clean the data   \n",
    "    def preprocessData(self, examples):\n",
    "        clean_data = []\n",
    "        for i in range(len(examples)):\n",
    "            #convert lower case\n",
    "            sentence = examples[i][1].lower()\n",
    "           \n",
    "            #remove punctuation\n",
    "            sentence = re.sub('[^a-zA-Z0-9\\']+', ' ', sentence)\n",
    "            sentence = re.sub(r'\\s+',' ', sentence)\n",
    "            clean_data.append((examples[i][0], sentence, examples[i][2]))\n",
    "        return clean_data\n",
    "\n",
    "    #helper function to do streaming\n",
    "    def tokenizer_stemming(self,data):\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        return [lemmatizer.lemmatize(word) for word in data]\n",
    "\n",
    "            \n",
    "         \n",
    "    def train(self, examples):\n",
    "       \n",
    "        #get clean data\n",
    "        clean_data = self.preprocessData(examples)\n",
    "        \n",
    "        for i in range(len(clean_data)):\n",
    "            \n",
    "            word_vectors = set(self.featurize(clean_data[i][1]))\n",
    "#             print(set(word_vectors))\n",
    "#             sentences = self.tokenizer_stemming(word_vectors)\n",
    "\n",
    "            for word in word_vectors:\n",
    "               \n",
    "                if not word[0] in self.vocab_list:\n",
    "                    self.vocab_list[word[0]] = 1\n",
    "                else:\n",
    "                    self.vocab_list[word[0]] +=1\n",
    "           \n",
    "        # remove stopword\n",
    "        stop = stopwords.words('english')\n",
    "        for word, value in self.vocab_list.items():\n",
    "            if not word in stop:\n",
    "                self.vocab_reduced[word] = value\n",
    "        \n",
    "        # group by the class\n",
    "        for i in range(len(clean_data)):\n",
    "            \n",
    "            #count the number class one in document\n",
    "            self.NumClass['1'] += clean_data[i][2]=='1'\n",
    "            \n",
    "            #count the number class zero in document\n",
    "            self.NumClass['0'] += clean_data[i][2]=='0'\n",
    "            \n",
    "            if clean_data[i][2] == '1':\n",
    "                word_vectors = set(self.featurize(clean_data[i][1]))\n",
    "#                 sentences = self.tokenizer_stemming(clean_data[i][1])\n",
    "#                 sentences = set(self.tokenizer_stemming(word_vectors))\n",
    "                for word in word_vectors:\n",
    "                    if word[0] in self.vocab_reduced:\n",
    "                        self.freqPos[word[0]] +=1\n",
    "                    else:\n",
    "                        continue\n",
    "            else:\n",
    "#                 sentences = self.tokenizer_stemming(clean_data[i][1])\n",
    "                word_vectors = set(self.featurize(clean_data[i][1]))\n",
    "#                 sentences = set(self.tokenizer_stemming(word_vectors))\n",
    "                for word in  word_vectors:\n",
    "                    if word[0] in self.vocab_reduced:\n",
    "                        self.freqNeg[word[0]] +=1\n",
    "                    else:\n",
    "                        continue\n",
    "#         print(self.vocab_list)\n",
    "        #get tokens word from frequecy postive \n",
    "        self.tokenPostive = sum(self.freqPos.values())\n",
    "        \n",
    "        #get tokens word from frequecy Negative \n",
    "        self.tokenNegative = sum(self.freqNeg.values())\n",
    "        \n",
    "        #number of document\n",
    "        self.NumDocu = len(examples)\n",
    "        \n",
    "       \n",
    "        #calculate the prior of each class and add into dictionary\n",
    "        self.prior['1'] = self.NumClass['1']/self.NumDocu\n",
    "        self.prior['0'] = self.NumClass['0']/self.NumDocu\n",
    "        \n",
    "        #calcualte the likelihood of each class and add into dictionatry\n",
    "        for word in self.vocab_reduced:\n",
    "            if word in self.freqPos:\n",
    "                likelihood_one = (self.freqPos[word] + 1) / (self.tokenPostive + len(self.vocab_reduced))\n",
    "                self.likelihood[(word,'1')] = likelihood_one\n",
    "            else: \n",
    "                likelihood_one = ( 0 + 1) / (self.tokenPostive + len(self.vocab_reduced))\n",
    "                self.likelihood[(word,'1')] = likelihood_one\n",
    "\n",
    "            \n",
    "            if word in self.freqNeg:\n",
    "                likelihood_zero = (self.freqNeg[word] + 1) / (self.tokenNegative + len(self.vocab_reduced))\n",
    "                self.likelihood[(word,'0')] = likelihood_zero\n",
    "            else:\n",
    "                likelihood_zero = (0 + 1) / (self.tokenNegative + len(self.vocab_reduced))\n",
    "                self.likelihood[(word,'0')] = likelihood_zero\n",
    "        \n",
    "        \n",
    "    def score(self, data):\n",
    "        words = data.split()\n",
    "        p_data_one = 0.0\n",
    "        p_data_zero = 0.0\n",
    "        likelihood_one = 0\n",
    "        likelihood_zero = 0\n",
    "        for w in words:\n",
    "            if w in self.vocab_reduced:\n",
    "                likelihood_one += np.log(self.likelihood[(w,'1')])\n",
    "                likelihood_zero += np.log(self.likelihood[(w,'0')])\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        p_data_one = np.exp(np.log(self.prior['1']) + likelihood_one)\n",
    "        p_data_zero = np.exp(np.log(self.prior['0']) + likelihood_zero)\n",
    "        \n",
    "        return { '1' : p_data_one, '0' : p_data_zero}\n",
    "\n",
    "    def classify(self, data):\n",
    "        dict_class = self.score(data)\n",
    "        if dict_class['1'] > dict_class['0']:\n",
    "            return '1'\n",
    "        else:\n",
    "            return '0'\n",
    "\n",
    "    def featurize(self, data):\n",
    "        words = data.split()\n",
    "        word_vectors = []\n",
    "        for w in words:\n",
    "            word_vectors.append((w,True))\n",
    "\n",
    "        return word_vectors\n",
    "            \n",
    "\n",
    "    def __str__(self):\n",
    "        return \"NAME FOR YOUR CLASSIFIER HERE\"\n",
    "    \n",
    "    #extra credit generate ramdom data for training and development and test\n",
    "    def k_fold(self, all_exaples, k):\n",
    "        foldSize = int(len(all_exaples)/k)\n",
    "        data_split = []\n",
    "        data_copy = list(all_exaples)\n",
    "        for idx in range(k):\n",
    "            sent_split= []\n",
    "            while len(sent_split) < foldSize:\n",
    "                randomIndex = np.random.randint(len(data_copy))\n",
    "                sentence = data_copy.pop(randomIndex)\n",
    "                sent_split.append(sentence)\n",
    "            data_split.append(sent_split)\n",
    "        return data_split\n",
    "\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "implement logistic regression class for extra credit\n",
    "\"\"\" \n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.weight = {}\n",
    "        self.freqPos = Counter()\n",
    "        self.freqNeg = Counter()\n",
    "        \n",
    "    def train(self, exampleText):\n",
    "        clean_data = self.preprocessData(exampleText)\n",
    "    \n",
    "    #helper function to clean the data   \n",
    "    def preprocessData(self, examples):\n",
    "        clean_data = []\n",
    "        for i in range(len(examples)):\n",
    "            #convert lower case\n",
    "            sentence = examples[i][1].lower()\n",
    "            #remove punctuation\n",
    "            sentence = re.sub('[^a-zA-Z0-9\\']+', ' ', sentence)\n",
    "            sentence = re.sub(r'\\s+',' ', sentence)\n",
    "            clean_data.append((examples[i][0], sentence, examples[i][2]))\n",
    "        return clean_data\n",
    "    \n",
    "    def featurize(self, data):\n",
    "        words = data.split()\n",
    "        word_vectors = []\n",
    "        for w in words:\n",
    "            word_vectors.append((w,True))\n",
    "\n",
    "        return word_vectors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     if len(sys.argv) != 3:\n",
    "#         print(\"Usage:\", \"python hw3_sentiment.py training-file.txt testing-file.txt\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "#     training = sys.argv[1]\n",
    "#     testing = sys.argv[2]\n",
    "\n",
    "    sa = SentimentAnalysis()\n",
    "    print(sa)\n",
    "    # do the things that you need to with your base class\n",
    "    sa.train(exampleText)\n",
    "    \n",
    "    #get the development text from file and calculate the classify each document\n",
    "    exampleDev_text = generate_tuples_from_file('dev_file.txt')\n",
    "    dev_list = []\n",
    "    gold_labels = []\n",
    "    for i in range(len(exampleDev_text)):\n",
    "        dev_list.append(exampleDev_text[i][1])\n",
    "        gold_labels.append(exampleDev_text[i][2])\n",
    "    \n",
    "   \n",
    "\n",
    "    # classify for each document\n",
    "    classify_labels = []\n",
    "    for data in dev_list:\n",
    "        classify_labels.append(sa.classify(data))\n",
    "    print(\"SentimanetAnaysis Model\")\n",
    "    print(\"Recall    : {}\".format(recall(gold_labels, classify_labels)))\n",
    "    print(\"Precision : {}\".format(precision(gold_labels, classify_labels)))\n",
    "    print(\"F1 Score  : {}\".format(f1(gold_labels, classify_labels)))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    improved = SentimentAnalysisImproved()\n",
    "    print(improved)\n",
    "    exampleText = generate_tuples_from_file('train_file.txt')\n",
    "    improved.train(exampleText)\n",
    "    \n",
    "    exampleDev_text = generate_tuples_from_file('dev_file.txt')\n",
    "    dev_list = []\n",
    "    gold_labels = []\n",
    "    for i in range(len(exampleDev_text)):\n",
    "        dev_list.append(exampleDev_text[i][1])\n",
    "        gold_labels.append(exampleDev_text[i][2])\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    # classify for each document\n",
    "    classify_labels = []\n",
    "    for data in dev_list:\n",
    "        classify_labels.append(improved.classify(data))\n",
    "    print(\"SentimanetAnaysis Model\")\n",
    "    print(\"Recall    : {}\".format(recall(gold_labels, classify_labels)))\n",
    "    print(\"Precision : {}\".format(precision(gold_labels, classify_labels)))\n",
    "    print(\"F1 Score  : {}\".format(f1(gold_labels, classify_labels)))\n",
    "    \n",
    "    \n",
    "    # do the things that you need to with your improved class\n",
    "    combineDataset = createNewDataset('train_file.txt', 'dev_file.txt')\n",
    "    k_dataset = improved.k_fold(combineDataset, 10)\n",
    "    \n",
    "    #print the recall, precision and f1 from k fold\n",
    "#     print_result_score(improved, k_dataset, 10)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement helper functions\n",
    "\"\"\"\n",
    "def generate_tuples_from_file(training_file_path):\n",
    "    dataset = pd.read_csv(training_file_path)\n",
    "    list_tuple = []\n",
    "    for i in range(len(dataset[\"id\"])):\n",
    "        tuple_word = (dataset['id'][i], dataset['text'][i], dataset['author'][i])\n",
    "        list_tuple.append(tuple_word)\n",
    "    return list_tuple\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "o\n",
      "I\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "list1 = [('com', 'o'), ('am', 'I'), ('bong', 'o')]\n",
    "list2 = [('com', 'o'), ('bong', 'I'), ('bongs', 'O')]\n",
    "count = 0\n",
    "for i in range(len(list1)):\n",
    "    if list1[i] == list2[i]:\n",
    "        count = count + 1\n",
    "        print(count)\n",
    "    print(list1[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chakryaros/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chakryaros/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"When I was a child , I wasn't allowed to watch Sesame Street !\"]\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"When I was a child , I wasn't allowed to watch Sesame Street !\"\n",
    "sentences = sent_tokenize(paragraph)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'I', 'was', 'a', 'child', ',', 'I', 'was', \"n't\", 'allowed', 'to', 'watch', 'Sesame', 'Street', '!']\n"
     ]
    }
   ],
   "source": [
    "sentences = \"When I was a child, I wasn't allowed to watch Sesame Street!\"\n",
    "words = word_tokenize(sentences)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'I', 'was', 'a', 'child', ',', 'I', 'was', \"n't\", 'allowed', 'to', 'watch', 'Sesame', 'Street', '!']\n"
     ]
    }
   ],
   "source": [
    "sentences = \"When I was a child , I was n't allowed to watch Sesame Street !\"\n",
    "words = word_tokenize(sentences)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer(language = 'english')\n",
    "def stemming(words):\n",
    "    new = []\n",
    "    stem_words = [snowball.stem(x) for x in words]\n",
    "    new.append(stem_words)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['when',\n",
       "  'i',\n",
       "  'was',\n",
       "  'a',\n",
       "  'child',\n",
       "  ',',\n",
       "  'i',\n",
       "  'was',\n",
       "  \"n't\",\n",
       "  'allow',\n",
       "  'to',\n",
       "  'watch',\n",
       "  'sesam',\n",
       "  'street',\n",
       "  '!']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['When', 'I', 'wa', 'a', 'child', ',', 'I', 'wa', \"n't\", 'allowed', 'to', 'watch', 'Sesame', 'Street', '!']]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatization(words):\n",
    "    new = []\n",
    "    lem_words = [lemmatizer.lemmatize(x) for x in words]\n",
    "    new.append(lem_words)\n",
    "    return new\n",
    "\n",
    "new = lemmatization(words)\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
