{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework, make sure that you format your notbook nicely and cite all sources in the appropriate sections. Programmatically generate or embed any figures or graphs that you need.\n",
    "\n",
    "Names: \n",
    " - Chakrya Ros\n",
    " - Sara Timermans Pastor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Word2Vec paper questions\n",
    "---------------------------\n",
    "\n",
    "(answer the questions here)\n",
    "\n",
    "1. A CBOW word embedding takes the context of each word as the input and tries to predict the word in the context. For example, I am doing NPL homework, assume the input to the Neural Network to the word, \"NPL\", so we try to predict a target word, \"homework\". In training model, we need to use the one hot encoding to the input word and measure the output error compared to one hod encoding of the taret word \"homework\".\n",
    "\n",
    "2. The different between CBOW and Skip-gram word embedding:\n",
    "\n",
    "    - In CBOW, the current word is predicted using the window of surrouding context windows. For instance, if $w_{i- \n",
    "1}, w_{i-2}, w_{i-3},w_{i+1}, w_{i+2}, w_{i+3}$ are given words or context, this model will give $w_{i}$. It's fast training and work better on frequency words. \n",
    "\n",
    "    - In Skip Gram, it predicts the given sequence or context from the word. It's opposite of CBOW. For instance, if $w_{i}$ is given, this will predict the context, $w_{i-1}, w_{i-2}, w_{i-3},w_{i+1}, w_{i+2}, w_{i+3}$. It's slow training and work better on infrequency words.\n",
    "\n",
    "\n",
    "3. The task that the authors use to evaluate the generated word embeddings are to train CBOW and Skip-gram models on corpora with one trillion words, especially, unlimited size of vocabulary., RNN vectors are used with other techniquest to achieve over 50 percent.\n",
    "\n",
    "\n",
    "4. PCA and t-SNE:\n",
    "\n",
    "    - PCA is a dimension reduciton tool that can be helped to reduce a large set of variables to a small set and still contains most of the information in the original set.\n",
    "\n",
    "    - t-SNE (t-Distributed Stochastic Neighbor Embedding) is a technique for dimemsionality reduction and is well suited for the visualization of high-dimensional datasets.\n",
    "\n",
    "    They are important to the task of training and interpreting word embeddings because word embeddings model trains on the very large dataset and very large dimension word vector like 100 to 300 dimensions. So , PCA and t-SNE are the good tools to use reduce the dimemsion and can help use visualize on the graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources Cited\n",
    "--------------------------\n",
    "Cite all sources that you consulted to answer these questions here, including textbooks, papers, online resources, friends, etc.\n",
    "\n",
    "- https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "- https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-8-dimensionality-reduction-chi2-pca-c6d06fb3fcf3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: Training your own word embeddings\n",
    "--------------------------------\n",
    "\n",
    "(describe the Spooky Authors Dataset here)\n",
    "\n",
    "\n",
    "Describe what data set you have chosen to compare and contrast with the Spooky Authors Dataset. Make sure to describe where it comes from and it's general properties.\n",
    "\n",
    "We take data set from gutenberg project. We use nltk.coprus to get datasets. We decided to combine five differece texts into our whole dataset, shakespeare-caesar.txt, shakespeare-hamlet.txt, shakespeare-macbeth.txt,' austen-emma.txt, austen-persuasion.txt. The different between Spoopy Authors dataset and our dataset is the Spooky Authors dataset is non-fiction , factual and reports on true events. Our datasets are about fiction and novels that are based on  the author's imagination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chakryaros/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import your libraries here\n",
    "\n",
    "import nltk\n",
    "#preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize   # tokenizing\n",
    "from nltk.stem import PorterStemmer  # using the Porter Stemmer\n",
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()\n",
    "nltk.download('stopwords')\n",
    "#library for create dataset\n",
    "import urllib\n",
    "import bs4 as bs\n",
    "import csv\n",
    "#training model\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import numpy as np, array\n",
    "from numpy import argmax\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "# % matplotlib inline\n",
    "\n",
    "# feedforward model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Embedding, SimpleRNN\n",
    "from keras.preprocessing import sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to train your word embeddings\n",
    "\n",
    "class CBOW_Model:\n",
    "    def __init__(self):\n",
    "        self.corpus = []\n",
    "        self.text = None\n",
    "        self.author = None\n",
    "    \n",
    "    def train(self, dataset):\n",
    "        df = pd.read_csv(dataset)\n",
    "        self.text = df['text']\n",
    "    \n",
    "        # iterate through each sentence in cleanText\n",
    "        for i in range(len(self.text)):\n",
    "            cleanText = preprocessing(self.text[i])\n",
    "            \n",
    "            #tokenize the sentence into words\n",
    "            self.corpus.append(word_tokenize(cleanText))\n",
    "    \n",
    "        \n",
    "        # build vocabulary and train model\n",
    "        # size is the dimensionality of the feature vectors. \n",
    "        # window is the maximum distance between target word and its neighboring word\n",
    "        # min_count is minimum frequency count of words, model would ignore the word\n",
    "        # if it's less than min_count\n",
    "        # workers = how many threads to use behind the scense\n",
    "        # iter = number of iterations (epochs) over the corpus\n",
    "        CBOW_mode = gensim.models.Word2Vec(self.corpus, min_count=2, size=150, window = 5, workers =4, iter=10)\n",
    "        \n",
    "        return CBOW_mode\n",
    "    \n",
    "#create my own dataset\n",
    "def createDataset():\n",
    "    # Gettings the data source\n",
    "    text1 = gutenberg.raw('shakespeare-caesar.txt')\n",
    "    text2 = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "    text3 = gutenberg.raw('shakespeare-macbeth.txt')\n",
    "    text4 = gutenberg.raw('austen-emma.txt')\n",
    "    text5 = gutenberg.raw('austen-persuasion.txt')\n",
    "    text = text1 + text2 + text3 + text4 + text5\n",
    "    \n",
    "    # Preprocessing the data\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d',' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    \n",
    "    # convert the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    #write into file\n",
    "    with open('ourDataset.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['text'])\n",
    "        \n",
    "        for sen in sentences:\n",
    "            writer.writerow([sen])\n",
    "            \n",
    "createDataset()\n",
    "\n",
    "    \n",
    "#function to visualize the model       \n",
    "def tsne_plot_visualize(title, words, model):\n",
    "    \n",
    "    embeddingClusters = []\n",
    "    wordClusters = []\n",
    "    for word in words:\n",
    "        embedding = []\n",
    "        similarWord = []\n",
    "        # find the similar word and add into similarword list\n",
    "        for S_word, percent in model.wv.most_similar(word, topn=10):\n",
    "            similarWord.append(S_word)\n",
    "            # get word encoding word\n",
    "            embedding.append(model.wv[S_word])\n",
    "        \n",
    "        embeddingClusters.append(embedding)\n",
    "        wordClusters.append(similarWord)\n",
    "    \n",
    "    #convert embedding to numpy array\n",
    "    embeddingClusters = np.array(embeddingClusters)\n",
    "    #get the shape of embedding cluster\n",
    "    x, y, z = embeddingClusters.shape\n",
    "    tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=2500, random_state=32)\n",
    "    embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embeddingClusters.reshape(x * y, z))).reshape(x, y, 2)\n",
    "    \n",
    "    \n",
    "    #plot the figure\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(words)))\n",
    "    for label, embeddings, words, color in zip(words, embeddings_en_2d, wordClusters, colors):\n",
    "        x = embeddings[:, 0]\n",
    "        y = embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=0.7, label=label)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.7, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='top', size=8)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "# clean the dataset, remove space and convert to lower case\n",
    "def preprocessing(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \", text)\n",
    "    word_tokens = text.lower().split()\n",
    "\n",
    "    #remove stopwords\n",
    "    stopWord = stopwords.words('english')\n",
    "    word_tokens = [word for word in word_tokens if not word in stopWord]\n",
    "\n",
    "    cleanText = \" \".join(word_tokens)\n",
    "    return cleanText\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size of Skoopy dataset :  15662\n",
      "[('swede', 0.9102968573570251), ('usages', 0.9080865979194641), ('gouty', 0.8851004838943481), ('misanthrope', 0.8823884129524231), ('hovers', 0.877611517906189), ('old', 0.8650044202804565), ('bugs', 0.8648256063461304), ('sensibilities', 0.8625751733779907), ('woman', 0.8624029159545898), ('credited', 0.861343264579773)]\n",
      "CBOW model find similarity between 'man' and 'woman':  0.86240304\n"
     ]
    }
   ],
   "source": [
    "#Skoopy Authors Dataset using CBOW Model\n",
    "wb = CBOW_Model()\n",
    "cbow_model = wb.train('skoopy.csv')\n",
    "\n",
    "#get the vocabulary from model\n",
    "vocabs = list(cbow_model.wv.vocab)\n",
    "print(\"vocabulary size of Skoopy dataset : \", len(vocabs))\n",
    "#get encoding of word of 100 dimemsion\n",
    "word_vectors = cbow_model.wv['love']\n",
    "\n",
    "#words to display on the graph of the similar words in this list\n",
    "words = ['dinner', 'happiness', 'man', 'sat', 'illness', 'day', 'home', 'two']\n",
    "\n",
    "#find the most similar word \n",
    "w = cbow_model.wv.most_similar(['man'])\n",
    "w1 = cbow_model.wv.most_similar(['dinner'])\n",
    "print(w)\n",
    "\n",
    "#similarity between two differen word\n",
    "print(\"CBOW model find similarity between 'man' and 'woman': \",\n",
    "       cbow_model.wv.similarity(w1=\"man\", w2=\"woman\"))\n",
    "\n",
    "# tsne_plot_visualize(\"Skoopy Authors Dataset Using CBOW Model\", words, cbow_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size of Emma dataset :  7827\n",
      "Most similar word to 'man' [('woman', 0.941717803478241), ('fastidious', 0.9289137125015259), ('young', 0.8742282390594482), ('incomprehensible', 0.8662596344947815), ('bestride', 0.8376628756523132), ('concise', 0.8369568586349487), ('thoughtless', 0.835969090461731), ('intreat', 0.8333361148834229), ('rated', 0.8331584930419922), ('artful', 0.832138180732727)]\n"
     ]
    }
   ],
   "source": [
    "#austen-emma dataset from nltk using CBOW Model\n",
    "wb = CBOW_Model()\n",
    "cb = wb.train('ourDataset.csv')\n",
    "\n",
    "# get vocabulary from CBOW model\n",
    "vocabs = list(cb.wv.vocab)\n",
    "print(\"vocabulary size of Emma dataset : \", len(vocabs))\n",
    "mydata_words = ['dinner', 'happiness', 'man', 'sat', 'illness', 'day', 'home', 'two']\n",
    "#find the most similar word \n",
    "word = cb.wv.most_similar(['man'])\n",
    "print(\"Most similar word to 'man'\", word)\n",
    "\n",
    "#similarity between two differen word\n",
    "# print(\"CBOW model find similarity between 'cat' and 'dog': \",\n",
    "#        cb.wv.similarity(w1=\"cat\", w2=\"dog\"))\n",
    "\n",
    "# tsne_plot_visualize(\"My own Dataset Using CBOW Model\", mydata_words, cb)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources Cited\n",
    "--------------------------\n",
    "Cite all sources that you consulted to answer these questions here, including textbooks, papers, online resources, friends, etc.\n",
    "\n",
    "- https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "- https://radimrehurek.com/gensim/models/word2vec.html\n",
    "- https://towardsdatascience.com/google-news-and-leo-tolstoy-visualizing-word2vec-word-embeddings-with-t-sne-11558d8bd4d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 3: Evaluate the differences between the word embeddings\n",
    "----------------------------\n",
    "\n",
    "(make sure to include graphs, figures, and paragraphs with full sentences)\n",
    "\n",
    "We ues CBOW to train these two different dataset. One is for skoopy author dataset and another one is our own dataset that we take from texts from the Project Gutenberg. The skoopy's dataset has 15662 vocabulary words and our own dataset have 7827 vacaulary words. After trainging these two datasets, we seleced some words like dinner', 'happiness', 'man', 'sat', 'illness', 'day', 'home', 'two from both models and find their most similar and draw the graph to see the different.\n",
    "\n",
    "By looking at the first graph, the most top 10 similar words to 'dinner' are des, copy, letters, conversation, accident voyage, population, ride, shortly and moskoe.\n",
    "\n",
    "By looking at the second graph, the most top 10 similar words to 'dinner' are visitor, early, town, pause, except, nessessarily,grateful, metioning, news and got. \n",
    "\n",
    "By compare the most top 10 similar words to 'dinner' in those two datasets, we can see none of their similar words are the same. The season that they don't have the same similar words because the neighbors to the word 'dinner' in these two dataset are different. \n",
    "\n",
    "More than that, in the skoopy author dataset, the first graph, the group of similar words by the colors stay closer to each other, but our dataset , the group of similar words by the colors do not quite stay closer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources Cited\n",
    "--------------------------\n",
    "Cite all sources that you consulted to answer these questions here, including textbooks, papers, online resources, friends, etc.\n",
    "\n",
    "- https://www.guru99.com/word-embedding-word2vec.html\n",
    "- https://code.google.com/archive/p/word2vec/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 4: Feedforward Neural Language Model\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to train a feedforward neural language model \n",
    "# on a set of given word embeddings\n",
    "# make sure not to just copy + paste to train your two \n",
    "\n",
    "class FeedforwardNeural:\n",
    "    \n",
    "    def __init__(self, x,y):\n",
    "        \n",
    "        self.x_train = x     # input\n",
    "        self.y_train = y    # output\n",
    "\n",
    "    \n",
    "    #forward propagation through our network\n",
    "    def train(self):\n",
    "        \n",
    "        # set up the basis for a feed forward network\n",
    "        model = Sequential()\n",
    "        \n",
    "        # set up three layers\n",
    "        model.add(Dense(units= 100, activation='relu', input_dim=self.x_train.shape[1]))\n",
    "       \n",
    "        model.add(Dense(units= 50, activation='relu'))\n",
    "\n",
    "        model.add(Dense(units=self.y_train.shape[1], activation='softmax'))\n",
    "\n",
    "        \n",
    "        \n",
    "        # configure the learning process\n",
    "        model.compile(loss='binary_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "        \n",
    "     \n",
    "        # fit the model\n",
    "        history = model.fit(self.x_train, self.y_train, epochs=150, verbose=1, batch_size=35)\n",
    "        #batch size is responsible for how many samples we want to \n",
    "        #use in one epoch, which means how many samples are used \n",
    "        #in one forward/backward pass.\n",
    "        \n",
    "\n",
    "        return model, history\n",
    "    \n",
    "\n",
    "def oneHot_encode(data):\n",
    "    \n",
    "    # integer encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(data)\n",
    "\n",
    "    # binary encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    y_train_encode = onehot_encoder.fit_transform(integer_encoded)\n",
    "    \n",
    "    return y_train_encode\n",
    "\n",
    "def get_word_embedding_split_dataset(wb_model, N):\n",
    "    y_train = []\n",
    "    x_train = []\n",
    "    vocabs = list(wb_model.wv.vocab)\n",
    "\n",
    "    # get the words from word embedding model\n",
    "    for i in range(3, len(vocabs)):\n",
    "        wb_i_1 = wb_model.wv[vocabs[i - N]]\n",
    "        wb_i_2 = wb_model.wv[vocabs[i - N + 1]]\n",
    "        wb_i_3 = wb_model.wv[vocabs[i - N + 2]]\n",
    "        wb_i = wb_i_1 + wb_i_2 + wb_i_3\n",
    "        x_train.append(wb_i)\n",
    "        # get the word from vocab list\n",
    "        y_train.append(vocabs[i])\n",
    "\n",
    "    x_train = np.array(x_train)    \n",
    "    y_train = np.array(y_train) \n",
    "\n",
    "    X_train, X_test, Y_train, y_test = train_test_split(x_train, y_train, test_size=0.25, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, Y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11744, 150)\n",
      "(11744, 11744)\n",
      "(3915, 3915)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "11744/11744 [==============================] - 11s 919us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 2/150\n",
      "11744/11744 [==============================] - 10s 847us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 3/150\n",
      "11744/11744 [==============================] - 9s 782us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 4/150\n",
      "11744/11744 [==============================] - 9s 774us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 5/150\n",
      "11744/11744 [==============================] - 8s 640us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 6/150\n",
      "11744/11744 [==============================] - 7s 591us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 7/150\n",
      "11744/11744 [==============================] - 7s 624us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 8/150\n",
      "11744/11744 [==============================] - 8s 670us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 9/150\n",
      "11744/11744 [==============================] - 8s 688us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 10/150\n",
      "11744/11744 [==============================] - 6s 553us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 11/150\n",
      "11744/11744 [==============================] - 6s 552us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 12/150\n",
      "11744/11744 [==============================] - 9s 752us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 13/150\n",
      "11744/11744 [==============================] - 9s 731us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 14/150\n",
      "11744/11744 [==============================] - 7s 636us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 15/150\n",
      "11744/11744 [==============================] - 7s 620us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 16/150\n",
      "11744/11744 [==============================] - 7s 619us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 17/150\n",
      "11744/11744 [==============================] - 7s 633us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 18/150\n",
      "11744/11744 [==============================] - 8s 643us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 19/150\n",
      "11744/11744 [==============================] - 7s 629us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 20/150\n",
      "11744/11744 [==============================] - 9s 767us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 21/150\n",
      "11744/11744 [==============================] - 10s 876us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 22/150\n",
      "11744/11744 [==============================] - 9s 752us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 23/150\n",
      "11744/11744 [==============================] - 9s 752us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 24/150\n",
      "11744/11744 [==============================] - 10s 827us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 25/150\n",
      "11744/11744 [==============================] - 9s 728us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 26/150\n",
      "11744/11744 [==============================] - 8s 683us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 27/150\n",
      "11744/11744 [==============================] - 7s 572us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 28/150\n",
      "11744/11744 [==============================] - 8s 713us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 29/150\n",
      "11744/11744 [==============================] - 7s 562us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 30/150\n",
      "11744/11744 [==============================] - 7s 583us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 31/150\n",
      "11744/11744 [==============================] - 7s 556us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 32/150\n",
      "11744/11744 [==============================] - 7s 575us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 33/150\n",
      "11744/11744 [==============================] - 7s 623us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 34/150\n",
      "11744/11744 [==============================] - 9s 747us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 35/150\n",
      "11744/11744 [==============================] - 9s 734us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 36/150\n",
      "11744/11744 [==============================] - 8s 710us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 37/150\n",
      "11744/11744 [==============================] - 10s 822us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 38/150\n",
      "11744/11744 [==============================] - 8s 719us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 39/150\n",
      "11744/11744 [==============================] - 10s 817us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 40/150\n",
      "11744/11744 [==============================] - 9s 737us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 41/150\n",
      "11744/11744 [==============================] - 8s 700us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 42/150\n",
      "11744/11744 [==============================] - 8s 641us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 43/150\n",
      "11744/11744 [==============================] - 8s 641us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 44/150\n",
      "11744/11744 [==============================] - 8s 648us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 45/150\n",
      "11744/11744 [==============================] - 8s 643us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 46/150\n",
      "11744/11744 [==============================] - 8s 640us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 47/150\n",
      "11744/11744 [==============================] - 9s 750us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 48/150\n",
      "11744/11744 [==============================] - 8s 705us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 49/150\n",
      "11744/11744 [==============================] - 8s 723us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 50/150\n",
      "11744/11744 [==============================] - 10s 817us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 51/150\n",
      "11744/11744 [==============================] - 9s 800us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 52/150\n",
      "11744/11744 [==============================] - 9s 752us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 53/150\n",
      "11744/11744 [==============================] - 8s 711us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 54/150\n",
      "11744/11744 [==============================] - 8s 692us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 55/150\n",
      "11744/11744 [==============================] - 8s 667us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 56/150\n",
      "11744/11744 [==============================] - 8s 671us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 57/150\n",
      "11744/11744 [==============================] - 9s 771us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 58/150\n",
      "11744/11744 [==============================] - 8s 681us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 59/150\n",
      "11744/11744 [==============================] - 9s 762us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 60/150\n",
      "11744/11744 [==============================] - 8s 688us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 61/150\n",
      "11744/11744 [==============================] - 8s 686us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 62/150\n",
      "11744/11744 [==============================] - 8s 696us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 63/150\n",
      "11744/11744 [==============================] - 9s 744us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 64/150\n",
      "11744/11744 [==============================] - 9s 736us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 65/150\n",
      "11744/11744 [==============================] - 8s 709us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 66/150\n",
      "11744/11744 [==============================] - 9s 795us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 67/150\n",
      "11744/11744 [==============================] - 9s 785us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 68/150\n",
      "11744/11744 [==============================] - 11s 898us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 69/150\n",
      "11744/11744 [==============================] - 9s 778us/step - loss: 8.8288e-04 - accuracy: 0.99990s - loss: 8.828\n",
      "Epoch 70/150\n",
      "11744/11744 [==============================] - 9s 728us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 71/150\n",
      "11744/11744 [==============================] - 9s 794us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 72/150\n",
      "11744/11744 [==============================] - 9s 799us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 73/150\n",
      "11744/11744 [==============================] - 10s 843us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 74/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11744/11744 [==============================] - 9s 793us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 75/150\n",
      "11744/11744 [==============================] - 9s 786us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 76/150\n",
      "11744/11744 [==============================] - 8s 685us/step - loss: 8.8288e-04 - accuracy: 0.99990s - loss: 8.8288e-04 - ac\n",
      "Epoch 77/150\n",
      "11744/11744 [==============================] - 8s 684us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 78/150\n",
      "11744/11744 [==============================] - 9s 736us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 79/150\n",
      "11744/11744 [==============================] - 9s 781us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 80/150\n",
      "11744/11744 [==============================] - 10s 816us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 81/150\n",
      "11744/11744 [==============================] - 12s 1ms/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 82/150\n",
      "11744/11744 [==============================] - 9s 806us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 83/150\n",
      "11744/11744 [==============================] - 8s 688us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 84/150\n",
      "11744/11744 [==============================] - 7s 574us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 85/150\n",
      "11744/11744 [==============================] - 7s 616us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 86/150\n",
      "11744/11744 [==============================] - 7s 582us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 87/150\n",
      "11744/11744 [==============================] - 9s 730us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 88/150\n",
      "11744/11744 [==============================] - 9s 727us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 89/150\n",
      "11744/11744 [==============================] - 9s 767us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 90/150\n",
      "11744/11744 [==============================] - 10s 889us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 91/150\n",
      "11744/11744 [==============================] - 7s 598us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 92/150\n",
      "11744/11744 [==============================] - 8s 679us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 93/150\n",
      "11744/11744 [==============================] - 10s 809us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 94/150\n",
      "11744/11744 [==============================] - 10s 861us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 95/150\n",
      "11744/11744 [==============================] - 9s 736us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 96/150\n",
      "11744/11744 [==============================] - 8s 687us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 97/150\n",
      "11744/11744 [==============================] - 8s 706us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 98/150\n",
      "11744/11744 [==============================] - 8s 693us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 99/150\n",
      "11744/11744 [==============================] - 9s 743us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 100/150\n",
      "11744/11744 [==============================] - 10s 877us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 101/150\n",
      "11744/11744 [==============================] - 8s 660us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 102/150\n",
      "11744/11744 [==============================] - 7s 597us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 103/150\n",
      "11744/11744 [==============================] - 7s 596us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 104/150\n",
      "11744/11744 [==============================] - 7s 587us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 105/150\n",
      "11744/11744 [==============================] - 7s 594us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 106/150\n",
      "11744/11744 [==============================] - 8s 664us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 107/150\n",
      "11744/11744 [==============================] - 9s 795us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 108/150\n",
      "11744/11744 [==============================] - 10s 874us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 109/150\n",
      "11744/11744 [==============================] - 10s 852us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 110/150\n",
      "11744/11744 [==============================] - 9s 739us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 111/150\n",
      "11744/11744 [==============================] - 10s 837us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 112/150\n",
      "11744/11744 [==============================] - 9s 758us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 113/150\n",
      "11744/11744 [==============================] - 8s 681us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 114/150\n",
      "11744/11744 [==============================] - 9s 761us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 115/150\n",
      "11744/11744 [==============================] - 10s 849us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 116/150\n",
      "11744/11744 [==============================] - 7s 584us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 117/150\n",
      "11744/11744 [==============================] - 7s 634us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 118/150\n",
      "11744/11744 [==============================] - 10s 840us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 119/150\n",
      "11744/11744 [==============================] - 9s 731us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 120/150\n",
      "11744/11744 [==============================] - 9s 752us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 121/150\n",
      "11744/11744 [==============================] - 10s 825us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 122/150\n",
      "11744/11744 [==============================] - 8s 695us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 123/150\n",
      "11744/11744 [==============================] - 8s 675us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 124/150\n",
      "11744/11744 [==============================] - 8s 679us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 125/150\n",
      "11744/11744 [==============================] - 9s 777us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 126/150\n",
      "11744/11744 [==============================] - 8s 709us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 127/150\n",
      "11744/11744 [==============================] - 9s 760us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 128/150\n",
      "11744/11744 [==============================] - 9s 743us/step - loss: 8.8288e-04 - accuracy: 0.99990s - loss: 8.8288e-04 - accuracy: 0.\n",
      "Epoch 129/150\n",
      "11744/11744 [==============================] - 8s 682us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 130/150\n",
      "11744/11744 [==============================] - 9s 742us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 131/150\n",
      "11744/11744 [==============================] - 8s 657us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 132/150\n",
      "11744/11744 [==============================] - 8s 662us/step - loss: 8.8288e-04 - accuracy: 0.99991s\n",
      "Epoch 133/150\n",
      "11744/11744 [==============================] - 8s 666us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 134/150\n",
      "11744/11744 [==============================] - 8s 681us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 135/150\n",
      "11744/11744 [==============================] - 8s 653us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 136/150\n",
      "11744/11744 [==============================] - 8s 713us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 137/150\n",
      "11744/11744 [==============================] - 8s 660us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 138/150\n",
      "11744/11744 [==============================] - 8s 667us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 139/150\n",
      "11744/11744 [==============================] - 8s 666us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 140/150\n",
      "11744/11744 [==============================] - 8s 657us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 141/150\n",
      "11744/11744 [==============================] - 9s 801us/step - loss: 8.8288e-04 - accuracy: 0.99990s - loss:\n",
      "Epoch 142/150\n",
      "11744/11744 [==============================] - 10s 850us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 143/150\n",
      "11744/11744 [==============================] - 10s 843us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 144/150\n",
      "11744/11744 [==============================] - 10s 881us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 145/150\n",
      "11744/11744 [==============================] - 10s 829us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 146/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11744/11744 [==============================] - 9s 762us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 147/150\n",
      "11744/11744 [==============================] - 8s 713us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 148/150\n",
      "11744/11744 [==============================] - 8s 696us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 149/150\n",
      "11744/11744 [==============================] - 8s 641us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Epoch 150/150\n",
      "11744/11744 [==============================] - 7s 622us/step - loss: 8.8288e-04 - accuracy: 0.9999\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_101 (Dense)            (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 11744)             598944    \n",
      "=================================================================\n",
      "Total params: 619,094\n",
      "Trainable params: 619,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#train skoopy author data on feedforward\n",
    "wb = CBOW_Model()\n",
    "skoopy_data_model = wb.train('skoopy.csv')\n",
    "X_train, X_test, Y_train, y_test = get_word_embedding_split_dataset(skoopy_data_model, 3)\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "#encoding output\n",
    "y_train_encode = oneHot_encode(Y_train)\n",
    "\n",
    "print(y_train_encode.shape)\n",
    "\n",
    "\n",
    "ffw_model = FeedforwardNeural(X_train, y_train_encode)\n",
    "ffw, history = ffw_model.train()\n",
    "print(ffw.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14128, 150)\n",
      "(14128,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "14128/14128 [==============================] - 13s 904us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 2/150\n",
      "14128/14128 [==============================] - 14s 998us/step - loss: 7.4696e-04 - accuracy: 0.9999s - loss: 7.4696e-0\n",
      "Epoch 3/150\n",
      "14128/14128 [==============================] - 12s 842us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 4/150\n",
      "14128/14128 [==============================] - 12s 873us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 5/150\n",
      "14128/14128 [==============================] - 13s 921us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 6/150\n",
      "14128/14128 [==============================] - 12s 857us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 7/150\n",
      "14128/14128 [==============================] - 12s 867us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 8/150\n",
      "14128/14128 [==============================] - 12s 858us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 9/150\n",
      "14128/14128 [==============================] - 12s 872us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 10/150\n",
      "14128/14128 [==============================] - 12s 872us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 11/150\n",
      "14128/14128 [==============================] - 13s 888us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 12/150\n",
      "14128/14128 [==============================] - 13s 917us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 13/150\n",
      "14128/14128 [==============================] - 13s 906us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 14/150\n",
      "14128/14128 [==============================] - 13s 890us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 15/150\n",
      "14128/14128 [==============================] - 12s 876us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 16/150\n",
      "14128/14128 [==============================] - 13s 895us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 17/150\n",
      "14128/14128 [==============================] - 12s 883us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 18/150\n",
      "14128/14128 [==============================] - 13s 902us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 19/150\n",
      "14128/14128 [==============================] - 13s 896us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 20/150\n",
      "14128/14128 [==============================] - 12s 884us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 21/150\n",
      "14128/14128 [==============================] - 13s 953us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 22/150\n",
      "14128/14128 [==============================] - 13s 892us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 23/150\n",
      "14128/14128 [==============================] - 13s 885us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 24/150\n",
      "14128/14128 [==============================] - 12s 884us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 25/150\n",
      "14128/14128 [==============================] - 12s 848us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 26/150\n",
      "14128/14128 [==============================] - 13s 898us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 27/150\n",
      "14128/14128 [==============================] - 12s 875us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 28/150\n",
      "14128/14128 [==============================] - 12s 883us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 29/150\n",
      "14128/14128 [==============================] - 13s 905us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 30/150\n",
      "14128/14128 [==============================] - 13s 896us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 31/150\n",
      "14128/14128 [==============================] - 12s 830us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 32/150\n",
      "14128/14128 [==============================] - 11s 799us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 33/150\n",
      "14128/14128 [==============================] - 12s 853us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 34/150\n",
      "14128/14128 [==============================] - 12s 860us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 35/150\n",
      "14128/14128 [==============================] - 66s 5ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 36/150\n",
      "14128/14128 [==============================] - 11s 753us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 37/150\n",
      "14128/14128 [==============================] - 11s 773us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 38/150\n",
      "14128/14128 [==============================] - 482s 34ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 39/150\n",
      "14128/14128 [==============================] - 11s 779us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 40/150\n",
      "14128/14128 [==============================] - 12s 884us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 41/150\n",
      "14128/14128 [==============================] - 14s 963us/step - loss: 7.4696e-04 - accuracy: 0.9999s - loss: 7.4696e-04 - accu\n",
      "Epoch 42/150\n",
      "14128/14128 [==============================] - 15s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 43/150\n",
      "14128/14128 [==============================] - 13s 955us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 44/150\n",
      "14128/14128 [==============================] - 13s 933us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 45/150\n",
      "14128/14128 [==============================] - 14s 964us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 46/150\n",
      "14128/14128 [==============================] - 21s 2ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 47/150\n",
      "14128/14128 [==============================] - 11s 777us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 48/150\n",
      "14128/14128 [==============================] - 13s 932us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 49/150\n",
      "14128/14128 [==============================] - 13s 927us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 50/150\n",
      "14128/14128 [==============================] - 15s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 51/150\n",
      "14128/14128 [==============================] - 11s 795us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 52/150\n",
      "14128/14128 [==============================] - 14s 984us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 53/150\n",
      "14128/14128 [==============================] - 13s 905us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 54/150\n",
      "14128/14128 [==============================] - 20s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 55/150\n",
      "14128/14128 [==============================] - 11s 779us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 56/150\n",
      "14128/14128 [==============================] - 14s 972us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 57/150\n",
      "14128/14128 [==============================] - 13s 926us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 58/150\n",
      "14128/14128 [==============================] - 15s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 59/150\n",
      "14128/14128 [==============================] - 14s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 60/150\n",
      "14128/14128 [==============================] - 14s 957us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 61/150\n",
      "14128/14128 [==============================] - 18s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 62/150\n",
      "14128/14128 [==============================] - 74s 5ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 63/150\n",
      "14128/14128 [==============================] - 75s 5ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 64/150\n",
      "14128/14128 [==============================] - 26s 2ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 65/150\n",
      "14128/14128 [==============================] - 12s 859us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 66/150\n",
      "14128/14128 [==============================] - 12s 882us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 67/150\n",
      "14128/14128 [==============================] - 12s 868us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 68/150\n",
      "14128/14128 [==============================] - 15s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 69/150\n",
      "14128/14128 [==============================] - 12s 815us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 70/150\n",
      "14128/14128 [==============================] - 13s 913us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 71/150\n",
      "14128/14128 [==============================] - 12s 876us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 72/150\n",
      "14128/14128 [==============================] - 23s 2ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 73/150\n",
      "14128/14128 [==============================] - 11s 766us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 74/150\n",
      "14128/14128 [==============================] - 13s 905us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 75/150\n",
      "14128/14128 [==============================] - 13s 886us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 76/150\n",
      "14128/14128 [==============================] - 14s 998us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 77/150\n",
      "14128/14128 [==============================] - 18s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 78/150\n",
      "14128/14128 [==============================] - 13s 925us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 79/150\n",
      "14128/14128 [==============================] - 48s 3ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 80/150\n",
      "14128/14128 [==============================] - 10s 727us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 81/150\n",
      "14128/14128 [==============================] - 12s 814us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 82/150\n",
      "14128/14128 [==============================] - 13s 891us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 83/150\n",
      "14128/14128 [==============================] - 13s 896us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 84/150\n",
      "14128/14128 [==============================] - 14s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 85/150\n",
      "14128/14128 [==============================] - 14s 964us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 86/150\n",
      "14128/14128 [==============================] - 12s 849us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 87/150\n",
      "14128/14128 [==============================] - 12s 836us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 88/150\n",
      "14128/14128 [==============================] - 60s 4ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 89/150\n",
      "14128/14128 [==============================] - 11s 749us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 90/150\n",
      "14128/14128 [==============================] - 13s 891us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 91/150\n",
      "14128/14128 [==============================] - 12s 884us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 92/150\n",
      "14128/14128 [==============================] - 14s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 93/150\n",
      "14128/14128 [==============================] - 13s 940us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 94/150\n",
      "14128/14128 [==============================] - 13s 896us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 95/150\n",
      "14128/14128 [==============================] - 12s 873us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 96/150\n",
      "14128/14128 [==============================] - 14s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 97/150\n",
      "14128/14128 [==============================] - 14s 984us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 98/150\n",
      "14128/14128 [==============================] - 13s 955us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 99/150\n",
      "14128/14128 [==============================] - 16s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 100/150\n",
      "14128/14128 [==============================] - 67s 5ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 101/150\n",
      "14128/14128 [==============================] - 77s 5ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 102/150\n",
      "14128/14128 [==============================] - 30s 2ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 103/150\n",
      "14128/14128 [==============================] - 11s 807us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 104/150\n",
      "14128/14128 [==============================] - 13s 921us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 105/150\n",
      "14128/14128 [==============================] - 13s 927us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 106/150\n",
      "14128/14128 [==============================] - 14s 995us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 107/150\n",
      "14128/14128 [==============================] - 13s 885us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 108/150\n",
      "14128/14128 [==============================] - 12s 879us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 109/150\n",
      "14128/14128 [==============================] - 13s 898us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 110/150\n",
      "14128/14128 [==============================] - 294s 21ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 111/150\n",
      "14128/14128 [==============================] - 19s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 112/150\n",
      "14128/14128 [==============================] - 13s 951us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 113/150\n",
      "14128/14128 [==============================] - 11s 746us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 114/150\n",
      "14128/14128 [==============================] - 12s 818us/step - loss: 7.4696e-04 - accuracy: 0.9999s - l\n",
      "Epoch 115/150\n",
      "14128/14128 [==============================] - 19s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 116/150\n",
      "14128/14128 [==============================] - 19s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 117/150\n",
      "14128/14128 [==============================] - 16s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 118/150\n",
      "14128/14128 [==============================] - 11s 802us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 119/150\n",
      "14128/14128 [==============================] - 12s 839us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 120/150\n",
      "14128/14128 [==============================] - 12s 856us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 121/150\n",
      "14128/14128 [==============================] - 11s 809us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 122/150\n",
      "14128/14128 [==============================] - 12s 855us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 123/150\n",
      "14128/14128 [==============================] - 14s 969us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 124/150\n",
      "14128/14128 [==============================] - 13s 909us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 125/150\n",
      "14128/14128 [==============================] - 15s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 126/150\n",
      "14128/14128 [==============================] - 16s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 127/150\n",
      "14128/14128 [==============================] - 11s 774us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 128/150\n",
      "14128/14128 [==============================] - 14s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 129/150\n",
      "14128/14128 [==============================] - 11s 752us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 130/150\n",
      "14128/14128 [==============================] - 14s 958us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 131/150\n",
      "14128/14128 [==============================] - 10s 743us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 132/150\n",
      "14128/14128 [==============================] - 11s 773us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 133/150\n",
      "14128/14128 [==============================] - 13s 894us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 134/150\n",
      "14128/14128 [==============================] - 13s 950us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 135/150\n",
      "14128/14128 [==============================] - 13s 942us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 136/150\n",
      "14128/14128 [==============================] - 15s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 137/150\n",
      "14128/14128 [==============================] - 15s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 138/150\n",
      "14128/14128 [==============================] - 12s 878us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 139/150\n",
      "14128/14128 [==============================] - 15s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 140/150\n",
      "14128/14128 [==============================] - 21s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 141/150\n",
      "14128/14128 [==============================] - 18s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 142/150\n",
      "14128/14128 [==============================] - 14s 988us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 143/150\n",
      "14128/14128 [==============================] - 14s 990us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 144/150\n",
      "14128/14128 [==============================] - 13s 952us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 145/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14128/14128 [==============================] - 12s 851us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 146/150\n",
      "14128/14128 [==============================] - 17s 1ms/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 147/150\n",
      "14128/14128 [==============================] - 11s 769us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 148/150\n",
      "14128/14128 [==============================] - 11s 786us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 149/150\n",
      "14128/14128 [==============================] - 11s 789us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Epoch 150/150\n",
      "14128/14128 [==============================] - 11s 794us/step - loss: 7.4696e-04 - accuracy: 0.9999\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_83 (Dense)             (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 14128)             720528    \n",
      "=================================================================\n",
      "Total params: 740,678\n",
      "Trainable params: 740,678\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# train our own data on feedforward\n",
    "Ourdata_model = wb.train('ourDataset.csv')\n",
    "\n",
    "#get input for word embedding model\n",
    "X_train_OurData, X_test_OurData, Y_train_OurData, y_test_OurData = get_word_embedding_split_dataset(Ourdata_model, 3)\n",
    "print(X_train_OurData.shape)\n",
    "print(Y_train_OurData.shape)\n",
    "\n",
    "#encoding output\n",
    "y_train_OurData_encode = oneHot_encode(Y_train_OurData)\n",
    "\n",
    "ffw_model_ourData = FeedforwardNeural(X_train_OurData, y_train_OurData_encode)\n",
    "ffw_ourData = ffw_model_ourData.train()\n",
    "print(ffw_ourData.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3915, 11744)\n",
      "Skoopy Dataset Word Embeddings Perplexity : 1.0006121530720353\n",
      "Our Dataset Word Embeddings Perplexity : 1.0005178872753235\n",
      "\n",
      "Feed forward on Our Dataset Word Embeddings Model is better than Skoopy Dataset Word Embeddings Model\n",
      "Our Dateset Word Embeddings Model have Lower Perplexity.\n"
     ]
    }
   ],
   "source": [
    "def oneHot_decode(data, encode):\n",
    "    \n",
    "    # integer encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(data)\n",
    "\n",
    "\n",
    "    # binary encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    y_train_encode = onehot_encoder.fit_transform(integer_encoded)\n",
    "    inverted = []\n",
    "    \n",
    "    # invert first example\n",
    "    for i in range(len(encode)):\n",
    "        decode = label_encoder.inverse_transform([argmax(encode[i ,:])])\n",
    "       \n",
    "        inverted.append(decode[0])\n",
    "\n",
    "    \n",
    "    return inverted\n",
    "\n",
    "# make predictions with skoopy dataset\n",
    "vocabs = list(skoopy_data_model.wv.vocab)\n",
    "\n",
    "predictions_spooky = ffw.predict(X_test)\n",
    "print(predictions_skoopy.shape)\n",
    "\n",
    "\n",
    "# decode vector into word after prediction\n",
    "# inverted_predict_word = oneHot_decode(vocabs, predictions_skoopy)\n",
    "\n",
    "# make predictions with Our dataset\n",
    "vocabs_our = list(Ourdata_model.wv.vocab)\n",
    "predictions_OurData = ffw_ourData.predict(X_test_OurData)\n",
    "\n",
    "\n",
    "\n",
    "# decode vector into word after prediction\n",
    "# inverted_predict_word = oneHot_decode(vocabs_our, predictions_OurData)\n",
    "\n",
    "\n",
    "skoopy_perplexity = 2**(8.8288e-04) \n",
    "print(\"Skoopy Dataset Word Embeddings Perplexity :\" ,skoopy_perplexity)\n",
    "\n",
    "ourData_perplexity = 2**(7.4696e-04)\n",
    "print(\"Our Dataset Word Embeddings Perplexity :\" ,ourData_perplexity)\n",
    "print()\n",
    "if skoopy_perplexity < ourData_perplexity:\n",
    "    print(\"Feed forward on Skoopy Dataset Word Embeddings Model is better than Our Dataset Word Embeddings Model\")\n",
    "    print(\"Skoopy Dateset Word Embeddings Model have Lower Perplexity.\")\n",
    "\n",
    "else:\n",
    "    print(\"Feed forward on Our Dataset Word Embeddings Model is better than Skoopy Dataset Word Embeddings Model\")\n",
    "    print(\"Our Dateset Word Embeddings Model have Lower Perplexity.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources Cited\n",
    "--------------------------\n",
    "Cite all sources that you consulted to answer these questions here, including textbooks, papers, online \n",
    "resources, friends, etc.\n",
    "\n",
    "- https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras/\n",
    "- https://keras.io/\n",
    "- https://web.stanford.edu/~jurafsky/slp3/7.pdf\n",
    "- https://keras.io/callbacks/#csvlogger\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 5: Recurrent Neural Language Model\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to train a recurrent neural language model \n",
    "\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, x_train,y_train, x_test, y_test, vocabs):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.vocabs = vocabs\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        # size to cut texts after this number of words\n",
    "        maxlen = 80\n",
    "        batch_size = 32\n",
    "        \n",
    "        # reshape the input and output size\n",
    "        self.x_train = sequence.pad_sequences(self.x_train, maxlen=maxlen)\n",
    "        self.y_test = sequence.pad_sequences(self.y_test, maxlen=maxlen)\n",
    "        \n",
    "        ''' define the keras model '''\n",
    "        model = Sequential()\n",
    "        \n",
    "        #number of hidden units\n",
    "        model.add(SimpleRNN(units = 128, activation = \"relu\"))\n",
    "\n",
    "        # number of outputs\n",
    "        model.add(Dense(10))\n",
    "        \n",
    "        input_shape = (len(self.vocabs), 10, self.y_train.shape)\n",
    "        model.build(input_shape)\n",
    "        \n",
    "        \n",
    "\n",
    "        print(model.summary())\n",
    "        \n",
    "    \n",
    "        # fit model\n",
    "        model.fit(self.x_train, self.y_train, batch_size=35,epochs=15,\n",
    "          validation_data=(self.x_test, self.y_test))\n",
    "        \n",
    "        return model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11744, 150)\n"
     ]
    }
   ],
   "source": [
    "#train skoopy author data on RNN\n",
    "wb = CBOW_Model()\n",
    "skoopy_data_model = wb.train('skoopy.csv')\n",
    "vocabs = list(skoopy_data_model.wv.vocab)\n",
    "X_train_RNN, X_test_RNN, Y_train_RNN, y_test_RNN = get_word_embedding_split_dataset(skoopy_data_model, 3)\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "#encoding output\n",
    "y_train_RNN_encode = oneHot_encode(Y_train_RNN)\n",
    "print(y_train__RNN_encode.shape)\n",
    "y_test_RNN_encode = oneHot_encode(y_test_RNN)\n",
    "rnn = RNN(X_train, y_train_RNN_encode, X_test_RNN, y_test_RNN_encode, vocabs )\n",
    "rnn_model = rnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train our dataset on RNN\n",
    "Ourdata_model = wb.train('ourDataset.csv')\n",
    "X_train, X_test, Y_train, y_test = get_word_embedding_split_dataset(skoopy_data_model, 3)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "rnn = RNN(X_train, Y_train, X_test, y_test)\n",
    "rnn_model = rnn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources Cited\n",
    "--------------------------\n",
    "Cite all sources that you consulted to answer these questions here, including textbooks, papers, online resources, friends, etc.\n",
    "\n",
    "- https://www.tensorflow.org/tutorials/text/text_classification_rnn#train_the_model\n",
    "- https://stackoverflow.com/questions/38294046/simple-recurrent-neural-network-input-shape\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources Cited\n",
    "--------------------------\n",
    "Cite all sources that you consulted to answer these questions here, including textbooks, papers, online resources, friends, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
