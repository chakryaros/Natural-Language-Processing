{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Nets Workshop\n",
    "------------------\n",
    "CSCI 3832, Lecture 18, 2/24/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural net from scratch\n",
    "import numpy as np\n",
    "# seed random numbers so that you can \n",
    "# track the same numbers as each other\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# derivative of the sigmoid\n",
    "def sigmoid_deriv(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# to implement for question 5\n",
    "def relu(x):\n",
    "    pass\n",
    "\n",
    "# to implement (see eq'n 7.25 in text) for question 5\n",
    "def relu_deriv(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dataset\n",
    "# 3rd \"feature\" is the bias term\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# labels, transposed so that they match\n",
    "# easily with our inputs X\n",
    "y = np.array([[0,1,1,0]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What function does this dataset represent? __XOR__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[-0.25091976  0.90142861  0.46398788  0.19731697]\n",
      " [-0.68796272 -0.68801096 -0.88383278  0.73235229]\n",
      " [ 0.20223002  0.41614516 -0.95883101  0.9398197 ]]\n",
      "U: [[ 0.66488528]\n",
      " [-0.57532178]\n",
      " [-0.63635007]\n",
      " [-0.63319098]]\n"
     ]
    }
   ],
   "source": [
    "hidden_units = 4\n",
    "input_features = X.shape[1]\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "# TODO: fill in dimensions here for W and U\n",
    "# fill these in as a tuple like (rows, columns)\n",
    "W_dim = (input_features, hidden_units)\n",
    "# you'll need to use W_dim and U_dim to produce the\n",
    "# correct number of random numbers\n",
    "W = 2 * np.random.random(W_dim) - 1\n",
    "# note that we are doing binary classification\n",
    "U_dim = (hidden_units, 1)\n",
    "U = 2 * np.random.random(U_dim) - 1\n",
    "print(\"W:\", W)\n",
    "print(\"U:\", U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = X\n",
    "num_epochs = 1\n",
    "for i in range(num_epochs):\n",
    "    # forward propagation\n",
    "    h = sigmoid(np.dot(inputs,W))\n",
    "    y_hat = sigmoid(np.dot(h,U))\n",
    "\n",
    "    # how much did we miss?\n",
    "    layer2_error = y - y_hat\n",
    "    \n",
    "    # this is telling us how much to move\n",
    "    # our weights and in what direction\n",
    "    layer2_delta = layer2_error * sigmoid_deriv(y_hat)\n",
    "\n",
    "    # how much did each L1 value contribute to \n",
    "    # the L2 error (according to the weights)?\n",
    "    layer1_error = layer2_delta.dot(U.T)\n",
    "    \n",
    "    # this is telling us how much to move\n",
    "    # our weights and in what direction\n",
    "    layer1_delta = layer1_error * sigmoid_deriv(h)\n",
    "\n",
    "    U += h.T.dot(layer2_delta)\n",
    "    W += inputs.T.dot(layer1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Does the hidden layer have a bias term in this neural net? __YOUR ANSWER HERE__\n",
    "3. What variables' values change as the loop above iterates? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n"
     ]
    }
   ],
   "source": [
    "print(\"Output After Training:\")\n",
    "test_inputs = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "gold_labels = np.array([[0,1,1,0]]).T\n",
    "\n",
    "# TODO: write the code to assign labels to the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How many iterations did you need for the predicted values $\\hat y$ to match the actual values? __YOUR ANSWER HERE__\n",
    "5. Implement a `relu` and `relu_deriv` function. How many iterations do you need if the hidden layer's nonlinearity is ReLU instead of sigmoid? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Neural Nets from libraries\n",
    "----------------\n",
    "\n",
    "Now, we'll take a look at some common libraries used to create classifiers using neural nets. We'll take a look at [`keras`](https://keras.io/) which provides a nice API for implementing neural nets and can be run on top of TensorFlow, CNTK, or Theano. We'll look at an example using [`tensorflow`](https://github.com/tensorflow/tensorflow) as our backend.\n",
    "\n",
    "Installation of component libraries:\n",
    "\n",
    "```\n",
    "pip3 install tensorflow\n",
    "sudo pip3 install keras\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "(4, 1)\n",
      "Epoch 1/1\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.7702 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13f1c6b70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up the basis for a feed forward network\n",
    "model = Sequential()\n",
    "\n",
    "x_train = X\n",
    "y_train = y\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# set up our layers\n",
    "model.add(Dense(units=4, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# configure the learning process\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.55441314]\n",
      " [0.59873295]\n",
      " [0.5274918 ]\n",
      " [0.672054  ]]\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "y_test = np.array([[0,1,1,0]]).T\n",
    "labels = model.predict(x_test)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How does changing the number of hidden units affect the number of epochs needed for 100% accuracy? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interested in getting deeper into neural nets? \n",
    "\n",
    "\n",
    "Here are two places to start from:\n",
    "- take a look at the data that you can load from [`nltk`](https://www.nltk.org/data.html) and [`scikit-learn`](https://scikit-learn.org/stable/datasets/index.html#dataset-loading-utilities), then work on creating a neural net to do either binary or multinomial classification\n",
    "- take a look at the tensorflow + keras word embeddings tutorial [here](https://www.tensorflow.org/tutorials/text/word_embeddings). Note! This tutorial mentions RNNs, which are special kind of neural net (they are not the feedforward architecture that we've seen so far). We'll get into RNNs around the time of spring break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Create TSNE model and plot it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(18, 18)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "   \n",
    "tsne_plot(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train skoopy author data on feedforward\n",
    "skoopy_data_model = wb.train('skoopy.csv')\n",
    "\n",
    "#get words from word embedding CBOW\n",
    "vocabs =  list(skoopy_data_model.wv.vocab)\n",
    "y_train = []\n",
    "x_train = []\n",
    "for word in vocabs:\n",
    "    for similarWord, prob in skoopy_data_model.wv.similar_by_word(word, topn=1):\n",
    "        x_train.append(similarWord)\n",
    "        y_train.append(prob)\n",
    "\n",
    "x_train = np.array([x_train])\n",
    "y_train = np.array([y_train]).T\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "# ffw_model = FeedforwardNeural(x_train, y_train)\n",
    "# ffw = ffw_model.train()\n",
    "# ffw.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train skoopy author data on feedforward\n",
    "skoopy_data_model = wb.train('skoopy.csv')\n",
    "\n",
    "y_train = []\n",
    "x_train = []\n",
    "vocabs = list(skoopy_data_model.wv.vocab)\n",
    "N = 3\n",
    "\n",
    "# get the words from word embedding model\n",
    "for i in range(3, len(vocabs)):\n",
    "    wb_i_1 = skoopy_data_model.wv[vocabs[i - N]]\n",
    "    wb_i_2 = skoopy_data_model.wv[vocabs[i - N + 1]]\n",
    "    wb_i_3 = skoopy_data_model.wv[vocabs[i - N + 2]]\n",
    "    wb_i = wb_i_1 + wb_i_2 + wb_i_3\n",
    "    x_train.append(wb_i)\n",
    "    # get the word from vocab list\n",
    "    y_train.append(vocabs[i])\n",
    "    \n",
    "x_train = np.array(x_train)    \n",
    "y_train = np.array(y_train) \n",
    "\n",
    "y_train_encode = to_categorical(y_train, num_classes=len(vocabs), dtype='float32')    \n",
    "print(y_train_encode)\n",
    "# integer encode\n",
    "# label_encoder = LabelEncoder()\n",
    "# integer_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# # binary encode\n",
    "# onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "# y_train_encode = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# X_train, X_test, Y_train, y_test = train_test_split(x_train, y_train_encode, test_size=0.33, random_state=42)\n",
    "\n",
    "# ffw_model = FeedforwardNeural(X_train, Y_train)\n",
    "# ffw = ffw_model.train()\n",
    "# print(ffw.summary())\n",
    "\n",
    "# labels = model.predict(X_test)\n",
    "# print(labels)\n",
    "\n",
    "\n",
    "# # invert first example\n",
    "# inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "# print(inverted)\n",
    "\n",
    "\n",
    "\n",
    "# calculate Perplexity of test dataset\n",
    "# for word in inverted_predict_word:\n",
    "#     if word in vocabs:\n",
    "#         print(skoopy_data_model.wv.most_similar(word))\n",
    "#     else:\n",
    "#         continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
