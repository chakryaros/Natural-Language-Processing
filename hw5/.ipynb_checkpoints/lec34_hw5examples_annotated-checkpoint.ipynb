{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSCI 3832, Spring 2020, Lecture 34 — HW 5 examples & getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.special import softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: building a Mini-HMM\n",
    "========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyHMM:\n",
    "  \n",
    "    def __init__(self):\n",
    "        # here are the items that we'll need to gather\n",
    "        self.pi = Counter() # pi\n",
    "        self.transitions = {} # this is A\n",
    "        self.emissions = {} # this is B\n",
    "        self.states = set() # Q\n",
    "        self.vocab = set() # O\n",
    "\n",
    "    def train(self, example):\n",
    "        \"\"\"\n",
    "        Trains this model based on the given input data\n",
    "        params: example - a list of [(token, label), ....] tuples\n",
    "        return: None\n",
    "        \"\"\"\n",
    "        # we won't be implement Laplace smoothing today\n",
    "        \n",
    "        # pi * p(w_i | t_i)\n",
    "        \n",
    "        # prev_probability * p(w_i | t_i) * p(t_i | t_{i - 1})\n",
    "        \n",
    "        for i in range(len(example)):\n",
    "            word = example[i][0]\n",
    "            state = example[i][1]\n",
    "            if i == 0:\n",
    "                # count(sentences that start with state) / count of sentences\n",
    "                self.pi[state] += 1\n",
    "            else:\n",
    "                # p(t_i | t_{i - 1})\n",
    "                # count(t_i - 1, t_i) / count(t_i - 1)\n",
    "                # transitions[prev_state][state] = count(t_i - 1, t_i)\n",
    "                # sum(transitions[prev_state].values()) = count(t_i - 1)\n",
    "                if example[i - 1][1] not in self.transitions:\n",
    "                    self.transitions[example[i - 1][1]] = Counter()\n",
    "                self.transitions[example[i - 1][1]][state] += 1\n",
    "                \n",
    "                # TODO: think about what happens for the final state in the sentence\n",
    "                \n",
    "            # p(w_i | t_i)\n",
    "            # count(t_i, w_i) / count(t_i)\n",
    "            # emissions[state][word] = count(t_i, w_i)\n",
    "            # sum(emissions[state].values()) = count(t_i)\n",
    "            if state not in self.emissions:\n",
    "                self.emissions[state] = Counter()\n",
    "            self.emissions[state][word] += 1\n",
    "                \n",
    "        # TODO: make into percentages\n",
    "        print(self.pi)\n",
    "        print(self.transitions)\n",
    "        print(self.emissions)\n",
    "    \n",
    "    def greedy_decode(self, data):\n",
    "        \"\"\"\n",
    "        params: data - a list of tokens\n",
    "        return: a list of [(token, label)...] tuples\n",
    "        \"\"\"\n",
    "        # generate probabilities — run Viterbi\n",
    "        prev_prob = 0\n",
    "        prev_state = None\n",
    "        for i in range(len(data)):\n",
    "            word = data[i]\n",
    "            max_prob = 0\n",
    "            max_state = None\n",
    "            for state in self.emissions.keys():\n",
    "                emission = self.emissions[state][word] / sum(self.emissions[state].values())\n",
    "                if i == 0:\n",
    "                    # pi * emission prob\n",
    "                    pi_val = self.pi[state] / sum(self.pi.values())\n",
    "                    prob = emission * pi_val\n",
    "                else:\n",
    "                    transition = self.transitions[prev_state][state] / sum(self.transitions[prev_state].values())\n",
    "                    prob = prev_prob * transition * emission\n",
    "                \n",
    "                if max_state is None or prob > max_prob:\n",
    "                    max_state = state\n",
    "                    max_prob = prob\n",
    "            \n",
    "            prev_prob = max_prob\n",
    "            prev_state = max_state\n",
    "            print(word)\n",
    "            print(max_prob)\n",
    "            print(max_state)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = [(\"I\", \"NOUN\"), (\"went\", \"OTHER\"), (\"to\", \"OTHER\"), (\"the\", \"OTHER\"), (\"zoo\", \"NOUN\"), (\"and\", \"OTHER\"), \\\n",
    "               (\"I\", \"NOUN\"), (\"saw\", \"OTHER\"), (\"a\", \"OTHER\"), (\"penguin\", \"NOUN\"), (\"and\", \"OTHER\"), (\"sparkly\", \"OTHER\"), (\"jaguars\", \"NOUN\"), (\"!\", \"OTHER\")]\n",
    "train_data2 = [(\"Run\", \"OTHER\"), (\"away\", \"OTHER\")]\n",
    "test_data = \"I went to the penguin park .\".split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NOUN': 1})\n",
      "{'NOUN': Counter({'OTHER': 5}), 'OTHER': Counter({'OTHER': 4, 'NOUN': 4})}\n",
      "{'NOUN': Counter({'I': 2, 'zoo': 1, 'penguin': 1, 'jaguars': 1}), 'OTHER': Counter({'and': 2, 'went': 1, 'to': 1, 'the': 1, 'saw': 1, 'a': 1, 'sparkly': 1, '!': 1})}\n",
      "Counter({'NOUN': 1, 'OTHER': 1})\n",
      "{'NOUN': Counter({'OTHER': 5}), 'OTHER': Counter({'OTHER': 5, 'NOUN': 4})}\n",
      "{'NOUN': Counter({'I': 2, 'zoo': 1, 'penguin': 1, 'jaguars': 1}), 'OTHER': Counter({'and': 2, 'went': 1, 'to': 1, 'the': 1, 'saw': 1, 'a': 1, 'sparkly': 1, '!': 1, 'Run': 1, 'away': 1})}\n",
      "I\n",
      "0.2\n",
      "NOUN\n",
      "went\n",
      "0.018181818181818184\n",
      "OTHER\n",
      "to\n",
      "0.0009182736455463731\n",
      "OTHER\n",
      "the\n",
      "4.6377456845776425e-05\n",
      "OTHER\n",
      "penguin\n",
      "4.12244060851346e-06\n",
      "NOUN\n",
      "park\n",
      "0.0\n",
      "NOUN\n",
      ".\n",
      "0.0\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "hmm = GreedyHMM()\n",
    "hmm.train(train_data)\n",
    "hmm.train(train_data2)\n",
    "hmm.greedy_decode(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_probabilities(self, data):\n",
    "    \"\"\"\n",
    "    params: data - a list of tokens\n",
    "    return: two lists of dictionaries --\n",
    "      - first a list of dictionaries of states to probabilities, \n",
    "      one dictionary per word in the test data that represents the\n",
    "      probability of being at that state for that word\n",
    "      - second a list of dictionaries of states to states, \n",
    "      one dictionary per word in the test data that represents the \n",
    "      backpointers for which previous state led to the best probability\n",
    "      for the current state\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences between this HMM and the one that you're implementing for HW 5?\n",
    "- input params to train are different\n",
    "- you'll be implementing Viterbi\n",
    "- you'll be Laplace smoothing p(w_i | t_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: building a Mini-MEMM\n",
    "========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyMEMM:\n",
    "  \n",
    "    def __init__(self):\n",
    "        # TODO: randomly initialize your weights\n",
    "        self.weights = np.array([[1, 0, 1],[2, -3, -0.5],[0, 0.5, -1]])\n",
    "        #self.weights = 2 * np.random.random_sample((len(self.categories), self.num_feats + 1)) - 1\n",
    "        self.num_feats = 2\n",
    "        self.states = [1, 2, 3]\n",
    "\n",
    "    def train(self, examples, iterations = 100, learning_rate = 0.1):\n",
    "        \"\"\"\n",
    "        Trains this model based on the given input data\n",
    "        params: examples - a list of (features, label) data, one per example\n",
    "        return: None\n",
    "        \"\"\"\n",
    "        # run SGD\n",
    "        \n",
    "        # while we still need to iterate\n",
    "        # loop over our examples (in random order)\n",
    "            # get the feature representation\n",
    "            featurized = np.array(example[0] + [1]) # add the bias in\n",
    "            # calculate y_hat\n",
    "            z = np.dot(featurized, self.weights.T)\n",
    "            y_hat = softmax(z)\n",
    "            \n",
    "            # calculate gradients\n",
    "            # - (1 * {y = k} - p(y = k | x)) (then * x)\n",
    "            true_label = example[1]\n",
    "            indicators = [1 if j == true_label else 0 for j in self.states]\n",
    "            gradient_factors = -(1 * indicators -  y_hat)\n",
    "            gradients = gradient_factors.reshape(gradient_factors.shape[0], 1) @ featurized.reshape(1, featurized.shape[0])\n",
    "    \n",
    "            # update your weights according to the gradients multiplied by\n",
    "            # the learning rate\n",
    "    \n",
    "    def classify_single(self, data):\n",
    "        \"\"\"\n",
    "        params: data - a list of features\n",
    "        return: a list of [(token, label)...] tuples\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [([3, 0.75], 1), ([0, 1], 2), ([1, 1], 3), ([2, 4], 2)]\n",
    "test_data = [2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memm = GreedyMEMM()\n",
    "memm.train(train_data, iterations = 100)\n",
    "memm.classify_single(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences between this MEMM and the one that you're implementing for HW 5?\n",
    "- input params to train are different\n",
    "- you'll be implementing Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General questions:\n",
    "1. what happens with unknown words?\n",
    "    1. for the HMM? __We let them zero-out our p(w_i | t_i)__\n",
    "    2. for the MEMM? __they just won't \"turn on\" word-specific features. Same for unknown word shapes__\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW 5 format questions:\n",
    "1. What is the `generate_probabilities` function doing? __running Viterbi__\n",
    "    1. What do the return values represent? __the probability table and backpointer table__\n",
    "2. What is the `decode` function doing? __constructing the sentence + labels from the corresponding probability and backpointer tables__\n",
    "3. How to use the `precision`, `recall`, `f1` functions? __be careful with the format of the parameters!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
