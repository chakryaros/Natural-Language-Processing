{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Nets Workshop\n",
    "------------------\n",
    "CSCI 3832, Lecture 17, 2/24/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural net from scratch\n",
    "import numpy as np\n",
    "# seed random numbers so that you can \n",
    "# track the same numbers as each other\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# derivative of the sigmoid\n",
    "def sigmoid_deriv(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# to implement for question 5\n",
    "def relu(x):\n",
    "    pass\n",
    "\n",
    "# to implement (see eq'n 7.25 in text) for question 5\n",
    "def relu_deriv(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dataset\n",
    "# 3rd \"feature\" is the bias term\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# labels, transposed so that they match\n",
    "# easily with our inputs X\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What function does this dataset represent? it's XOR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[-0.25091976  0.90142861  0.46398788  0.19731697]\n",
      " [-0.68796272 -0.68801096 -0.88383278  0.73235229]\n",
      " [ 0.20223002  0.41614516 -0.95883101  0.9398197 ]]\n",
      "U: [[ 0.66488528]\n",
      " [-0.57532178]\n",
      " [-0.63635007]\n",
      " [-0.63319098]]\n"
     ]
    }
   ],
   "source": [
    "hidden_units = 4\n",
    "input_features = X.shape[1]\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "# TODO: fill in dimensions here for W and U\n",
    "W_dim = (input_features,hidden_units)\n",
    "W = 2 * np.random.random(W_dim) - 1\n",
    "# note that we are doing binary classification\n",
    "U_dim = (hidden_units,1)\n",
    "U = 2 * np.random.random(U_dim) - 1\n",
    "print(\"W:\", W)\n",
    "print(\"U:\", U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = X\n",
    "num_epochs = 1\n",
    "for i in range(num_epochs):\n",
    "    # forward propagation\n",
    "    h = sigmoid(np.dot(inputs,W))\n",
    "    y_hat = sigmoid(np.dot(h,U))\n",
    "\n",
    "    # how much did we miss?\n",
    "    layer2_error = y - y_hat\n",
    "    \n",
    "    # this is telling us how much to move\n",
    "    # our weights and in what direction\n",
    "    layer2_delta = layer2_error * sigmoid_deriv(y_hat)\n",
    "\n",
    "    # how much did each L1 value contribute to \n",
    "    # the L2 error (according to the weights)?\n",
    "    layer1_error = layer2_delta.dot(U.T)\n",
    "    \n",
    "    # this is telling us how much to move\n",
    "    # our weights and in what direction\n",
    "    layer1_delta = layer1_error * sigmoid_deriv(h)\n",
    "\n",
    "    U += h.T.dot(layer2_delta)\n",
    "    W += inputs.T.dot(layer1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Does the hidden layer have a bias term in this neural net? No\n",
    "3. What variables' values change as the loop above iterates? All the variables are changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n"
     ]
    }
   ],
   "source": [
    "print(\"Output After Training:\")\n",
    "test_inputs = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "gold_labels = np.array([[0,1,1,0]]).T\n",
    "\n",
    "# TODO: write the code to assign labels to the test data\n",
    "h = sigmoid(np.dot(test_inputs,W))\n",
    "y_hat = sigmoid(np.dot(h,U))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How many iterations did you need for the predicted values $\\hat y$ to match the actual values? __YOUR ANSWER HERE__\n",
    "5. Implement a `relu` and `relu_deriv` function. How many iterations do you need if the hidden layer's nonlinearity is ReLU instead of sigmoid? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Neural Nets from libraries\n",
    "----------------\n",
    "\n",
    "Now, we'll take a look at some common libraries used to create classifiers using neural nets. We'll take a look at [`keras`](https://keras.io/) which provides a nice API for implementing neural nets and can be run on top of TensorFlow, CNTK, or Theano. We'll look at an example using [`tensorflow`](https://github.com/tensorflow/tensorflow) as our backend.\n",
    "\n",
    "Installation of component libraries:\n",
    "\n",
    "```\n",
    "pip3 install tensorflow\n",
    "sudo pip3 install keras\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the basis for a feed forward network\n",
    "model = Sequential()\n",
    "\n",
    "x_train = X\n",
    "y_train = y\n",
    "\n",
    "# set up our layers\n",
    "model.add(Dense(units=4, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# configure the learning process\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "y_test = np.array([[0,1,1,0]]).T\n",
    "labels = model.predict(x_test)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How does changing the number of hidden units affect the number of epochs needed for 100% accuracy? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interested in getting deeper into neural nets? \n",
    "\n",
    "\n",
    "Here are two places to start from:\n",
    "- take a look at the data that you can load from [`nltk`](https://www.nltk.org/data.html) and [`scikit-learn`](https://scikit-learn.org/stable/datasets/index.html#dataset-loading-utilities), then work on creating a neural net to do either binary or multinomial classification\n",
    "- take a look at the tensorflow + keras word embeddings tutorial [here](https://www.tensorflow.org/tutorials/text/word_embeddings). Note! This tutorial mentions RNNs, which are special kind of neural net (they are not the feedforward architecture that we've seen so far). We'll get into RNNs around the time of spring break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
