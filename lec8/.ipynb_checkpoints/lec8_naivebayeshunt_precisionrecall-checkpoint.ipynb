{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSCI 3832: Lecture 8, investigating classifiers, precision, recall\n",
    "===========\n",
    "1/31/2020, Spring 2020, Muzny\n",
    "\n",
    "Relevant textbook sections: 4.1, 4.3, 4.7\n",
    "\n",
    "Today, we'll be spending our time investigating some classifiers that we've trained for you.\n",
    "\n",
    "All three of these classifiers are Naïve Bayes classifiers. For a given new, unlabeled document, they calculate:\n",
    "\n",
    "$$ P(feature_1, feature_2, feature_3, ..., feature_n | c)P(c)$$\n",
    "\n",
    "Where $c$ is a candidate class. They then select the class that has the highest probability to be the actual label of the new document.\n",
    "\n",
    "\n",
    "Task 1: Which Classifier is Which?\n",
    "-------------------------\n",
    "We have given you 3 Naïve Bayes classifiers. All three of these are binary classifiers that choose between the label '0' or '1' (these are strings).\n",
    "\n",
    "- one of these classifiers is an authorship attributor\n",
    "- one of these classifiers is a language identifier\n",
    "- one of these classifiers is a sentiment analyser\n",
    "\n",
    "Your first job is to conduct experiments to determine two things:\n",
    "1. Which classifier is which?\n",
    "2. What specific classes do you believe that they are choosing between? (what are better labels for each classifier than '0' and '1'?)\n",
    "    1. Note: this is a difficult task. It is of utmost importance that you consider the particular data set that they were trained on. I will tell you that they were trained using some of [nltk's available corpora](http://www.nltk.org/nltk_data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Chakrya Ros\n",
    "# Feel free to work in groups of 2 - 3/talk to your neighbors\n",
    "\n",
    "# You'll be turning this notebook in at the end of lecture today \n",
    "# as a pdf\n",
    "# File -> Download As -> .html -> open in a browser -> print to pdf\n",
    "# (one submission per group)\n",
    "# Please make a comment on your submission with your name and the name(s)\n",
    "# of your partners as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your trained classifiers from pickled files\n",
    "# (we've already trained your classifiers for you)\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt # for graphing\n",
    "#import nltk  # not necessary, but you can uncomment if you want\n",
    "\n",
    "# add more imports here as you would like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts a list of words so that they are featurized\n",
    "# for nltk's format for bag-of-words\n",
    "# params:\n",
    "# words - list of words where each element is a single word \n",
    "# return: dict mapping every word to True\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "f = open('classifier1.pickle', 'rb')\n",
    "classifier1 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('classifier2.pickle', 'rb')\n",
    "classifier2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('classifier3.pickle', 'rb')\n",
    "classifier3 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# in a list, if you find that helpful\n",
    "classifiers = [classifier1, classifier2, classifier3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['0', '1'])\n",
      "0.4366186617451996\n",
      "0.5633813382548005\n",
      "1\n",
      "dict_keys(['0', '1'])\n",
      "1.8810499402744543e-18\n",
      "0.9999999999999951\n",
      "1\n",
      "dict_keys(['0', '1'])\n",
      "0.46625312782388734\n",
      "0.5337468721761123\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Here's an example of how to run a test sentence through the classifiers\n",
    "# edit at your leisure\n",
    "test = \"All that glitters is not gold Fair is foul, and foul is fair: Hover through the fog and filthy air.\"\n",
    "# you can either split on whitespace or use nltk's word_tokenize\n",
    "featurized = word_feats(test.split()) \n",
    "for classifier in classifiers:\n",
    "    print(classifier.prob_classify(featurized).samples())  # will tell you what samples are available\n",
    "    print(classifier.prob_classify(featurized).prob('0'))  # get the probability for class '0'\n",
    "    print(classifier.prob_classify(featurized).prob('1'))  # get the probability for class '1'\n",
    "    print(classifier.classify(featurized))  # just get the label that it wants to assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['0', '1'])\n",
      "0.2115767801050558\n",
      "0.7884232198949435\n",
      "1\n",
      "dict_keys(['0', '1'])\n",
      "1.0847049928484364e-18\n",
      "0.9999999999999926\n",
      "1\n",
      "dict_keys(['0', '1'])\n",
      "0.9905583523654526\n",
      "0.009441647634547445\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# TODO: put in as many experiments as you'd like here (and feel free to add more cells as needed)\n",
    "# we recommend testing a variety of sentences. You can make these up or get them from sources\n",
    "# on the web\n",
    "# test = \"សួស្តីខ្ញុំជានិស្សិត\"\n",
    "test = \" RT @JohnGGalt: Amazing—after years of attacking Donald Trump the media managedto turn #InaugurationDay into all about themselves.\"\n",
    "# you can either split on whitespace or use nltk's word_tokenize\n",
    "featurized = word_feats(test.split()) \n",
    "for classifier in classifiers:\n",
    "    print(classifier.prob_classify(featurized).samples())  # will tell you what samples are available\n",
    "    print(classifier.prob_classify(featurized).prob('0'))  # get the probability for class '0'\n",
    "    print(classifier.prob_classify(featurized).prob('1'))  # get the probability for class '1'\n",
    "    print(classifier.classify(featurized))  # just get the label that it wants to assign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Answer the questions outlined at the beginning of this task here (please keep __bold__ formatting in this notebook):\n",
    "\n",
    "1. Which classifier is which?\n",
    "    1. classifier1 is __Sentiment Analysis__\n",
    "    1. classifier2 is __language identifier__\n",
    "    1. classifier3 is __authorship attributor__\n",
    "2. What specific classes do you believe that they are choosing between?\n",
    "    1. classifier1's '0' label should be __Negative__ and its '1' label should be __Positive__\n",
    "    1. classifier2's '0' label should be __Non-language identifier__ and its '1' label should be __language identifier__\n",
    "    1. classifier3's '0' label should be __Non-Shakespeare wrote__ and its '1' label should be __Shakespeare wrote__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Investigating Accuracy, Precision, and Recall\n",
    "---------------------------------------------\n",
    "Textbook: 4.7\n",
    "\n",
    "When we are determining how well a classifier is doing, we can look at overall accuracy:\n",
    "\n",
    "$$ accuracy = \\frac{true_{pos} + true_{neg}}{true_{pos} + false_{pos} + true_{neg} + false_{neg}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement this accuracy function, \n",
    "# then test the accuracy of two of the three classifiers from task 1.\n",
    "\n",
    "# Params: gold_labels, a list of labels assigned by hand (\"truth\")\n",
    "# predicted_labels, a corresponding list of labels predicted by the system\n",
    "# return: double accuracy (a number from 0 to 1)\n",
    "def accuracy(gold_labels, predicted_labels):\n",
    "    \n",
    "\n",
    "\n",
    "# test the accuracy of two of your classifiers.\n",
    "# Note: this requires knowing what labels your test data should have!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, (if you get this far).\n",
    "\n",
    "Often, however, it is more useful to look at __precision__ and __recall__ to determine how well a classifier is doing. This is especially important if we're dealing with imbalanced classes (one class occurs more frequently than another).\n",
    "\n",
    "$$ precision = \\frac{true_{pos}}{true_{pos} + false_{pos}} $$\n",
    "\n",
    "\n",
    "\n",
    "$$ recall = \\frac{true_{pos}}{true_{pos} + false_{neg}} $$\n",
    "\n",
    "To make this calculation, we'll need to choose which label is associated with \"positive\" and which is associated with \"negative\". For our purposes, we'll choose the label '1' to be our \"positive\" label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "1. Suppose you wanted a very precise system, but didn't care about recall. How would you achieve this?\n",
    "    1. __YOUR ANSWER HERE__\n",
    "\n",
    "2. Suppose you wanted a system with the best recall, but didn't care about precision. How would you achieve this?\n",
    "    1. __YOUR ANSWER HERE__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the precision and recall functions, \n",
    "# then test the precision/recall of two of the three classifiers from task 1.\n",
    "\n",
    "# Params: gold_labels, a list of labels assigned by hand (\"truth\")\n",
    "# predicted_labels, a corresponding list of labels predicted by the system\n",
    "# target_label (default value '1') - the label associated with \"positives\"\n",
    "# return: double precision (a number from 0 to 1)\n",
    "def precision(gold_labels, predicted_labels, target_label = '1'):\n",
    "    pass\n",
    "\n",
    "# Params: gold_labels, a list of labels assigned by hand (\"truth\")\n",
    "# predicted_labels, a corresponding list of labels predicted by the system\n",
    "# target_label (default value '1') - the label associated with \"positives\"\n",
    "# return: double recall (a number from 0 to 1)\n",
    "def recall(gold_labels, predicted_labels, target_label = '1'):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
